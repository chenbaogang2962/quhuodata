#### 清退预警

- fetch_dataset:
    template_code: ele_day_60
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [ 城市,团队ID,团队名称 ]
    ignore_null_error: true
    empty_df_record:
      城市: '-'
      团队ID: '-'
      团队名称: '-'
    rename:
      城市: coach_team_name
      团队ID: vendor_dc_id
      团队名称: dc_name
- run_py:
    - |
      df = df[df['vendor_dc_id'].notnull()]
      result = df
- drop_duplicates:
    subset: [ coach_team_name,vendor_dc_id ]
- push_dataset:
    key: coach_vendor_dc_id_map


- fetch_dataset:
    template_code:  ele_day_69   #清退预警--东战
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [站点ID,预警]
    ignore_null_error: true
    empty_df_record:
      站点ID: '-'
      预警: '-'
- df_rename_columns:
    - 站点ID: vendor_dc_id
      预警: stimulate_replenish_rectify
- run_py:
    - |
      df = to_df(df)
      df['stimulate_replenish_rectify_level'] = np.where((df['stimulate_replenish_rectify'].str.contains(u"安全")) | (df['stimulate_replenish_rectify'].str.contains(u"不参与")),u"安全",u"危险")
      result = to_dd(df)
- stash_push_df: []

- fetch_dataset:
    template_code:  ele_day_70    # 清退预警---运力覆盖
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [站点ID,异常站点运力覆盖]
    ignore_null_error: true
    empty_df_record:
      站点ID: '-'
      异常站点运力覆盖: '-'
- df_rename_columns:
    - 站点ID: vendor_dc_id
      异常站点运力覆盖: abnormal_dc_cover
- run_py:
    - |
      df = to_df(df)
      df['abnormal_dc_cover_level'] = np.where(df['abnormal_dc_cover'].str.contains(u"不"),u"安全",u"危险")
      result = to_dd(df)
- stash_push_df: []

- stash_join_df:
    on: [ vendor_dc_id ]
    how: outer
    drop_stash: true
- stash_push_df: []

- fetch_dataset:
    template_code: ele_day_71   # 清退预警--重庆（隔一日更新）
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [ 站点ID,考核结果预估]
    ignore_null_error: true
    empty_df_record:
      站点ID: '-'
      考核结果预估: '-'
- df_rename_columns:
    - 站点ID: vendor_dc_id
      考核结果预估: cq_dc_rectify
- run_py:
    - |
      df = to_df(df)
      df['cq_dc_rectify_level'] = np.where(df['cq_dc_rectify'].str.contains(u"需警惕"),u"危险",u"安全")
      result = to_dd(df)
- stash_push_df: []
- stash_join_df:
    on: [ vendor_dc_id ]
    how: outer
    drop_stash: true
- stash_push_df: []

- fetch_dataset:
    template_code:  ele_day_72   # 清退预警--上海（隔一日更新）
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [ 团队ID,是否触碰清退]
    ignore_null_error: true
    empty_df_record:
      团队ID: '-'
      是否触碰清退: '-'
- df_rename_columns:
    - 团队ID: vendor_dc_id
      是否触碰清退: sh_dc_optimize
- run_py:
    - |
      df = to_df(df)
      df['sh_dc_optimize_level'] = np.where(df['sh_dc_optimize'].str.contains(u"触碰清退"),u"危险",u"安全")
      result = to_dd(df)
- stash_push_df: []

- stash_join_df:
    on: [vendor_dc_id ]
    how: outer
    drop_stash: true
- stash_push_df: []
- use_df:
    key: coach_vendor_dc_id_map
- stash_push_df: []
- stash_join_df:
    on: vendor_dc_id
    how: inner
    drop_stash: true
- push_dataset:
    key:  clear_warning_mini

- use_df:
    key: clear_warning_mini
- run_py:
    - |
      df = to_df(df)
      df = df[df['sh_dc_optimize'].notnull()]
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label": "%s","level":"%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s"},{"name":"是否触碰清退","label":"%s","level":"%s"}]' % (a,b,c,d,e,f) for a,b,c,d,e,f in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','abnormal_dc_cover','abnormal_dc_cover_level','sh_dc_optimize','sh_dc_optimize_level']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: []

###重庆商圈
- use_df:
    key: clear_warning_mini
- run_py:
    - |
      df = to_df(df)
      df = df[df['cq_dc_rectify'].notnull()]
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label": "%s","level":"%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s"},{"name":"考核结果预估","label":"%s","level":"%s"}]' % (a,b,c,d,e,f) for a,b,c,d,e,f in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','abnormal_dc_cover','abnormal_dc_cover_level','cq_dc_rectify','cq_dc_rectify_level']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: []
- stash_concat_df:
    drop_stash: true
- stash_push_df: []
###非上海和重庆商圈
- use_df:
    key: clear_warning_mini
- run_py:
    - |
      df = to_df(df)
      df = df[(df['sh_dc_optimize'].isnull()) & (df['cq_dc_rectify'].isnull()) ]
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label": "%s","level":"%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s"}]' % (a,b,c,d) for a,b,c,d in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','abnormal_dc_cover','abnormal_dc_cover_level']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: []

- stash_concat_df:
    drop_stash: true
- drop_duplicates:
    subset: [ vendor_dc_id ]
- push_dataset:
    key: clear_warning_dc

- use_df:
    key: clear_warning_mini

- add_cols:
    - stimulate_replenish_rectify_level_value: 0
- df_set_column_val_if:
    column: stimulate_replenish_rectify_level_value
    condition: '[stimulate_replenish_rectify_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: stimulate_replenish_rectify_level_value
    condition: '[stimulate_replenish_rectify_level] == u"安全"'
    val: 3

- add_cols:
    - abnormal_dc_cover_level_value: 0
- df_set_column_val_if:
    column: abnormal_dc_cover_level_value
    condition: '[abnormal_dc_cover_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: abnormal_dc_cover_level_value
    condition: '[abnormal_dc_cover_level] == u"安全"'
    val: 3

- add_cols:
    - sh_dc_optimize_level_value: 0
- df_set_column_val_if:
    column: sh_dc_optimize_level_value
    condition: '[sh_dc_optimize_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: sh_dc_optimize_level_value
    condition: '[sh_dc_optimize_level] == u"安全"'
    val: 3

- add_cols:
    - cq_dc_rectify_level_value: 0
- df_set_column_val_if:
    column: cq_dc_rectify_level_value
    condition: '[cq_dc_rectify_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: cq_dc_rectify_level_value
    condition: '[cq_dc_rectify_level] == u"安全"'
    val: 3
- push_dataset:
    key: clear_warning_mini_std

- use_df:
    key: clear_warning_mini_std

- df_groupby:
    by: [ coach_team_name ]
- df_max:
    column: [ stimulate_replenish_rectify_level_value,abnormal_dc_cover_level_value,sh_dc_optimize_level_value,cq_dc_rectify_level_value]
- df_reset_index: [ ]
- df_rename_columns:
    - stimulate_replenish_rectify_level_value: srrlv
      abnormal_dc_cover_level_value: adclv
      sh_dc_optimize_level_value:  sdolv
      cq_dc_rectify_level_value: cdrlv
- stash_push_df: []
- use_df:
    key: clear_warning_mini_std
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: inner
    drop_stash: true
- push_dataset:
    key: clear_warning_mini_ext

- use_df:
    key:  clear_warning_mini_ext

#### stimulate_replenish_rectify
- df_select:
    - '[stimulate_replenish_rectify_level_value] == [srrlv]'

- fetch_cols:
    columns: [coach_team_name,dc_name,stimulate_replenish_rectify,stimulate_replenish_rectify_level]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'srrlv_dc_list'})
      result = to_dd(need)
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]

- fetch_cols:
    columns: [coach_team_name,stimulate_replenish_rectify,stimulate_replenish_rectify_level,srrlv_dc_list]

- push_dataset:
    key: clear_warning_city_stimulate_replenish_rectify

### abnormal_dc_cover
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[abnormal_dc_cover_level_value] == [adclv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name, abnormal_dc_cover,abnormal_dc_cover_level]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'adclv_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name, abnormal_dc_cover,abnormal_dc_cover_level,adclv_dc_list]
- push_dataset:
    key: clear_warning_city_abnormal_dc_cover

###sh_dc_optimize
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[sh_dc_optimize_level_value] == [sdolv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name,sh_dc_optimize,sh_dc_optimize_level ]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'sdolv_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name,sh_dc_optimize,sh_dc_optimize_level,sdolv_dc_list]

- push_dataset:
    key: clear_warning_sh_dc_optimize

#### cq_dc_rectify
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[cq_dc_rectify_level_value] == [cdrlv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name,cq_dc_rectify,cq_dc_rectify_level ]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'cdrlv_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name,cq_dc_rectify,cq_dc_rectify_level,cdrlv_dc_list ]
- stash_push_df: []

- use_df:
    key: clear_warning_city_stimulate_replenish_rectify
- stash_push_df: []
- stash_join_df:
    on: [ coach_team_name ]
    how: inner
    drop_stash: true
- stash_push_df: []

- use_df:
    key: clear_warning_city_abnormal_dc_cover
- stash_push_df: []
- stash_join_df:
    on: [ coach_team_name ]
    how: inner
    drop_stash: true
- stash_push_df: []
- use_df:
    key: clear_warning_sh_dc_optimize
- stash_push_df: [ ]

- stash_join_df:
    on: [ coach_team_name]
    how: inner
    drop_stash: true
- pad_cols:
    columns: [ srrlv_dc_list,adclv_dc_list,sdolv_dc_list,cdrlv_dc_list ]
    default_value: '-'
- push_dataset:
    key: clear_warning_city_mini

- use_df:
    key: clear_warning_city_mini
### 上海
- df_py_select:
    - 'df["coach_team_name"].str.contains(u"上海")'
- run_py:
    - |
      df = to_df(df)
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label": "%s","level":"%s","dc_list":"%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s","dc_list":"%s"},{"name":"是否触碰清退","label":"%s","level":"%s","dc_list":"%s"}]' % (a,b,c,d,e,f,g,h,i) for a,b,c,d,e,f,g,h,i in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','srrlv_dc_list','abnormal_dc_cover','abnormal_dc_cover_level','adclv_dc_list','sh_dc_optimize','sh_dc_optimize_level','sdolv_dc_list']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: [ ]

### 重庆
- use_df:
    key: clear_warning_city_mini
- df_py_select:
    - 'df["coach_team_name"].str.contains(u"重庆")'
- run_py:
    - |
      df = to_df(df)
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label":"%s","level":"%s","dc_list":"%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s","dc_list":"%s"},{"name":"考核结果预估","label":"%s","level":"%s","dc_list":"%s"}]' % (a,b,c,d,e,f,g,h,i) for a,b,c,d,e,f,g,h,i in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','srrlv_dc_list','abnormal_dc_cover','abnormal_dc_cover_level','adclv_dc_list','cq_dc_rectify','cq_dc_rectify_level','cdrlv_dc_list']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: [ ]

###非上海和重庆
- use_df:
    key: clear_warning_city_mini
- df_py_select:
    - '~((df["coach_team_name"].str.contains(u"重庆")) | (df["coach_team_name"].str.contains(u"上海")))'

- run_py:
    - |
      df = to_df(df)
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"预警","label": "%s","level":"%s","dc_list": "%s"},{"name":"异常站点运力覆盖","label": "%s","level":"%s","dc_list":"%s"}]' % (a,b,c,d,e,f) for a,b,c,d,e,f in  df[['stimulate_replenish_rectify','stimulate_replenish_rectify_level','srrlv_dc_list','abnormal_dc_cover','abnormal_dc_cover_level','adclv_dc_list']].to_dict(orient='split')['data'] ]
      result = df
- stash_push_df: [ ]

- stash_concat_df:
    drop_stash: true

- fetch_cols:
    columns: [coach_team_name,col_val_dict]
- add_cols:
    - dimension: 'C'
- stash_push_df: []
- use_df:
    key: ele_day_60_dc_std
    columns: [supplier_id,城市]
- df_rename_columns:
    - 城市: coach_team_name
- drop_duplicates:
    subset: [supplier_id,coach_team_name]
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: right
    drop_stash: true
- push_dataset:
    key: clear_warning_coach

- use_df:
    key: clear_warning_dc
- fetch_cols:
    columns: [coach_team_name,col_val_dict,vendor_dc_id]
- add_cols:
    - dimension: 'D'
- stash_push_df: []
- use_df:
    key: std_qplus_dc_copy
    columns: [supplier_id,vendor_dc_id]
- stash_push_df: []
- stash_join_df:
    on: [vendor_dc_id]
    how: inner
    drop_stash: true
- stash_push_df: []
- use_df:
    key: clear_warning_coach
- stash_push_df: []
- stash_concat_df:
    drop_stash: true
- set_meta_month_column:
    - book_month

- push_dataset:
    key: clean_warning_before


- fetch_dataset:
    template_code: ele_day_60
    dataset_cate: raw
    datakit_pull_way: last_day
    columns: [ 城市,团队ID,团队名称 ]
    ignore_null_error: true
    empty_df_record:
      城市: '-'
      团队ID: '-'
      团队名称: '-'
    rename:
      城市: coach_team_name
      团队ID: vendor_dc_id
      团队名称: dc_name
- run_py:
    - |
      df = df[df['vendor_dc_id'].notnull()]
      result = df
- drop_duplicates:
    subset: [ coach_team_name,vendor_dc_id ]
-  stash_push_df:  []

-  fetch_dataset:
     template_code : ele_day_79    #  ele站点考核项
     dataset_cate: raw
     datakit_pull_way: last_day
     ignore_null_error: true
     rename:
        站点ID: vendor_dc_id
        考核项1: evaluate_one
        考核项1的值: evaluate_one_value
        考核项2: evaluate_two
        考核项2的值: evaluate_two_value
        考核项3: evaluate_three
        考核项3的值: evaluate_three_value
        考核项4: evaluate_four
        考核项4的值: evaluate_four_value
- run_py:
    - |
      df = to_df(df)
      df['evaluate_one_level'] = np.where(df['evaluate_one_value'].str.contains(u"安全"),u"安全",np.where(df['evaluate_one_value'].str.contains(u'不参与'),'-',u'危险'))
      df['evaluate_two_level'] = np.where(df['evaluate_two_value'].str.contains(u"安全"),u"安全",np.where(df['evaluate_two_value'].str.contains(u'不参与'),'-',u'危险'))
      df['evaluate_three_level'] = np.where(df['evaluate_three_value'].str.contains(u"安全"),u"安全",np.where(df['evaluate_three_value'].str.contains(u'不参与'),'-',u'危险'))
      df['evaluate_four_level'] = np.where(df['evaluate_four_value'].str.contains(u"安全"),u"安全",np.where(df['evaluate_four_value'].str.contains(u'不参与'),'-',u'危险'))
      result = to_dd(df)
- stash_push_df: []
- stash_join_df:
    on: vendor_dc_id
    how: inner
    drop_stash: true

- push_dataset:
    key: clear_warning_mini


- use_df:
    key: clear_warning_mini
- run_py:
    - |
      df = to_df(df)
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"%s","level":"%s"},{"name":"%s","level":"%s"},{"name":"%s","level":"%s"},{"name":"%s","level":"%s"}]' % (a,b,c,d,e,f,g,h) for a,b,c,d,e,f,g,h in  df[['evaluate_one','evaluate_one_level','evaluate_two','evaluate_two_level','evaluate_three','evaluate_three_level','evaluate_four','evaluate_four_level']].to_dict(orient='split')['data'] ]
      result = df
- push_dataset:
    key: clear_warning_dc


- use_df:
    key: clear_warning_mini

- add_cols:
    - evaluate_one_level_value: 0
- df_set_column_val_if:
    column: evaluate_one_level_value
    condition: '[evaluate_one_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: evaluate_one_level_value
    condition: '[evaluate_one_level] == u"安全"'
    val: 3

- add_cols:
    - evaluate_two_level_value: 0
- df_set_column_val_if:
    column: evaluate_two_level_value
    condition: '[evaluate_two_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: evaluate_two_level_value
    condition: '[evaluate_two_level] == u"安全"'
    val: 3

- add_cols:
    - evaluate_three_level_value: 0
- df_set_column_val_if:
    column: evaluate_three_level_value
    condition: '[evaluate_three_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: evaluate_three_level_value
    condition: '[evaluate_three_level] == u"安全"'
    val: 3

- add_cols:
    - evaluate_four_level_value: 0
- df_set_column_val_if:
    column: evaluate_four_level_value
    condition: '[evaluate_four_level] == u"危险"'
    val: 4

- df_set_column_val_if:
    column: evaluate_four_level_value
    condition: '[evaluate_four_level] == u"安全"'
    val: 3
- push_dataset:
    key: clear_warning_mini_std
- use_df:
    key: clear_warning_mini_std

- df_groupby:
    by: [ coach_team_name ]
- df_max:
    column: [ evaluate_one_level_value,evaluate_two_level_value,evaluate_three_level_value,evaluate_four_level_value]
- df_reset_index: [ ]
- df_rename_columns:
    - evaluate_one_level_value: srrlv
      evaluate_two_level_value: adclv
      evaluate_three_level_value:  sdolv
      evaluate_four_level_value: cdrlv
- stash_push_df: []
- use_df:
    key: clear_warning_mini_std
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: inner
    drop_stash: true

- push_dataset:
    key: clear_warning_mini_ext


- use_df:
    key:  clear_warning_mini_ext

#### evaluate_one
- df_select:
    - '[evaluate_one_level_value] == [srrlv]'
- fetch_cols:
    columns: [coach_team_name,dc_name,evaluate_one,evaluate_one_level]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'evaluate_one_dc_list'})
      result = to_dd(need)
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]

- fetch_cols:
    columns: [coach_team_name,evaluate_one,evaluate_one_level,evaluate_one_dc_list]

- push_dataset:
    key: clear_warning_city_stimulate_replenish_rectify

### evaluate_two
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[evaluate_two_level_value] == [adclv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name, evaluate_two,evaluate_two_level]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'evaluate_two_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name,evaluate_two,evaluate_two_level,evaluate_two_dc_list]
- push_dataset:
    key: clear_warning_city_abnormal_dc_cover

###evaluate_three
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[evaluate_three_level_value] == [sdolv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name,evaluate_three,evaluate_three_level ]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'evaluate_three_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name,evaluate_three,evaluate_three_level,evaluate_three_dc_list]

- push_dataset:
    key: clear_warning_sh_dc_optimize

#### evaluate_four
- use_df:
    key: clear_warning_mini_ext
- df_select:
    - '[evaluate_four_level_value] == [cdrlv]'
- fetch_cols:
    columns: [ coach_team_name,dc_name,evaluate_four,evaluate_four_level]
- stash_push_df: []

- run_py:
    - |
      df = to_df(df)
      f = lambda x,y: u'_'.join([u'%s' % i for i in x[y]])
      need = df.groupby('coach_team_name',as_index=True).apply(f,y='dc_name').reset_index().rename(columns={0:u'evaluate_four_dc_list'})
      result = to_dd(need)
- stash_push_df: [ ]
- stash_join_df:
    on: [ coach_team_name ]
    how: right
    drop_stash: true
- drop_duplicates:
    - [ coach_team_name ]
- fetch_cols:
    columns: [ coach_team_name,evaluate_four,evaluate_four_level,evaluate_four_dc_list ]
- stash_push_df: []

- use_df:
    key: clear_warning_city_stimulate_replenish_rectify
- stash_push_df: []
- stash_join_df:
    on: [ coach_team_name ]
    how: inner
    drop_stash: true
- stash_push_df: []

- use_df:
    key: clear_warning_city_abnormal_dc_cover
- stash_push_df: []
- stash_join_df:
    on: [ coach_team_name ]
    how: inner
    drop_stash: true
- stash_push_df: []
- use_df:
    key: clear_warning_sh_dc_optimize
- stash_push_df: [ ]

- stash_join_df:
    on: [ coach_team_name]
    how: inner
    drop_stash: true
- pad_cols:
    columns: [ evaluate_one_dc_list,evaluate_two_dc_list,evaluate_three_dc_list,evaluate_four_dc_list ]
    default_value: '-'

- push_dataset:
    key: clear_warning_city_mini


- use_df:
    key: clear_warning_city_mini
- run_py:
    - |
      df = to_df(df)
      df['col_val_dict'] = '0'
      df['col_val_dict'] = [u'[{"name":"%s","level":"%s","dc_list":"%s"},{"name":"%s","level":"%s","dc_list":"%s"},{"name":"%s","level":"%s","dc_list":"%s"},{"name":"%s","level":"%s","dc_list":"%s"}]' % (a,b,c,d,e,f,g,h,i,j,k,l) for a,b,c,d,e,f,g,h,i,j,k,l in  df[['evaluate_one','evaluate_one_level','evaluate_one_dc_list','evaluate_two','evaluate_two_level','evaluate_two_dc_list','evaluate_three','evaluate_three_level','evaluate_three_dc_list','evaluate_four','evaluate_four_level','evaluate_four_dc_list']].to_dict(orient='split')['data'] ]
      result = df
- fetch_cols:
    columns: [coach_team_name,col_val_dict]
- add_cols:
    - dimension: 'C'
- stash_push_df: []
- use_df:
    key: ele_day_60_dc_std
    columns: [supplier_id,城市]
- df_rename_columns:
    - 城市: coach_team_name
- drop_duplicates:
    subset: [supplier_id,coach_team_name]
- stash_push_df: []
- stash_join_df:
    on: [coach_team_name]
    how: right
    drop_stash: true
- push_dataset:
    key: clear_warning_coach

- use_df:
    key: clear_warning_dc
- fetch_cols:
    columns: [coach_team_name,col_val_dict,vendor_dc_id]
- add_cols:
    - dimension: 'D'
- stash_push_df: []
- use_df:
    key: std_qplus_dc_copy
    columns: [supplier_id,vendor_dc_id]
- stash_push_df: []
- stash_join_df:
    on: [vendor_dc_id]
    how: inner
    drop_stash: true
- stash_push_df: []
- use_df:
    key: clear_warning_coach
- stash_push_df: []
- stash_concat_df:
    drop_stash: true
- set_meta_month_column:
    - book_month
- stash_push_df: []
- use_df:
    key: clean_warning_before
- stash_push_df: []

- stash_concat_df:
    drop_stash: true
